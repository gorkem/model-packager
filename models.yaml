- repo: meta-llama/Llama-2-7b
  name: llama-2
  family: llama
  variant: text
  parameterSize: 7b
  description: "Llama 2 7b text model"
  kitfileTemplate: ./kitfiles/llama2.yml
  quantization: f16
- repo: meta-llama/Llama-2-7b-chat
  name: llama-2
  parameterSize: 7b
  variant: chat
  description: "Llama 2 7b chat model"
  kitfileTemplate: ./kitfiles/llama2.yml
  quantization: f16
- repo: meta-llama/Meta-Llama-3-8B-Instruct
  name: llama3
  parameterSize: 8B
  variant: instruct
  description: "Llama 3 8B instruct model"
  kitfileTemplate: ./kitfiles/llama3.yml
  quantization: f16
  conversionFlags: "--vocab-type bpe"
- repo: meta-llama/Meta-Llama-3-8B
  name: llama3
  parameterSize: 8B
  variant: text
  description: "Llama 3 8B text model"
  kitfileTemplate: ./kitfiles/llama3.yml
  quantization: f16
  conversionFlags: "--vocab-type bpe"
- repo: meta-llama/Meta-Llama-3-70B-Instruct
  name: llama3
  parameterSize: 70B
  variant: instruct
  description: "Llama 3 70B instruct model"
  kitfileTemplate: ./kitfiles/llama3.yml
  quantization: f16
  conversionFlags: "--vocab-type bpe"
- repo: meta-llama/Meta-Llama-3-70B
  name: llama3
  parameterSize: 70B
  variant: text
  description: "Llama 3 70B text model"
  kitfileTemplate: ./kitfiles/llama3.yml
  quantization: f16
  conversionFlags: "--vocab-type bpe"