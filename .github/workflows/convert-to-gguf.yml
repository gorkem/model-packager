name: Convert to GGUF ModelKit

on:
  workflow_dispatch:
    inputs:
      model_repo:
          description: 'Huggingface repo'
          type: string
          required: true
      model_name:
          type: string
          description: 'Model name'
          required: true
      model_parameters:
          type: string
          description: 'Model parameter size'
          required: true
      model_variant:
          type: string
          description: 'Model variant'
          required: true
      model_qnt:
          type: string
          description: 'Model quantization'
          required: true
          default: 'f16'
      model_description:
          type: string
          description: 'Model description'
          required: true
      kitfile_template:
          type: string
          description: 'Kitfile template'
          required: true
      convert_flags:
          type: string
          description: 'Conversion flags'
          required: false
      runner: 
          type: string
          description: 'Runner'
          required: true
          default: 'model-factory-runner'

jobs:
    convert-model:
        runs-on: ${{ inputs.runner }}
        permissions:
          contents: read
          packages: write
        env:
            HF_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
            HF_HUB_ENABLE_HF_TRANSFER: 1
        steps:
            - uses: actions/checkout@v4
              with:
                fetch-depth: 0
            - uses: actions/setup-python@v5
              with:
                python-version: '3.10'
            - name: Install dependencies
              run:  |
                ./scripts/check-hf-cli
                ./scripts/check-convert
            - name: download model
              run: ./scripts/hf-clone ${{ inputs.model_repo }} ./model
            - name: check diskspace after download
              run: |
                echo "Disk space after base model downloads"
                df -h
            - name: convert model to gguf
              run: |
                ./llama.cpp/convert.py \
                ./model \
                --outfile ./model/${{inputs.model_name}}-${{inputs.model_parameters}}-${{inputs.model_variant}}-${{inputs.model_qnt}}.gguf \
                --outtype ${{inputs.model_qnt}} \
                ${{inputs.convert_flags}}
            - name: generate kitfile
              run: |
                  ./scripts/update-kitfile "${{inputs.model_name}}-${{inputs.model_parameters}}-${{inputs.model_variant}}-${{inputs.model_qnt}}" \
                  "${{inputs.model_description}}" ./${{inputs.model_name}}-${{inputs.model_parameters}}-${{inputs.model_variant}}-${{inputs.model_qnt}}.gguf \
                  ${{inputs.kitfile_template}} ./model/Kitfile
                  cat ./model/Kitfile
            - uses: jozu-ai/gh-kit-setup@v1.0.0
              name: install kit CLI
            - name: login to kit
              run: kit login ghcr.io --username ${{secrets.KIT_USER}} --password ${{secrets.KIT_PASSWORD}}
            - name: check diskspace before pack
              run: |
                echo "Disk space before packing model"
                df -h
            - name: pack modelkit
              run: |
                tree ./model
                kit pack ./model -t ghcr.io/jozu-ai/${{inputs.model_name}}:${{inputs.model_parameters}}-${{inputs.model_variant}}-${{inputs.model_qnt}} -v
            - name: push modelkit
              run: kit push ghcr.io/jozu-ai/${{inputs.model_name}}:${{inputs.model_parameters}}-${{inputs.model_variant}}-${{inputs.model_qnt}} -v
